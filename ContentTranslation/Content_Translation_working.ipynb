{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dOq1yuJ--zZD"
   },
   "outputs": [],
   "source": [
    "# # !gunzip cx-corpora.en2es.text.json.gz\n",
    "# !pip install --user pybind11\n",
    "# !pip install fasttext\n",
    "#!conda install -c conda-forge fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_w7cb_2j_U-S"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import fasttext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UsecK2xOBInn"
   },
   "outputs": [],
   "source": [
    "def apply_transform(vec, transform):\n",
    "    \"\"\"\n",
    "    Apply the given transformation to the vector space\n",
    "\n",
    "    Right-multiplies given transform with embeddings E:\n",
    "        E = E * transform\n",
    "\n",
    "    Transform can either be a string with a filename to a\n",
    "    text file containing a ndarray (compat. with np.loadtxt)\n",
    "    or a numpy ndarray.\n",
    "    \"\"\"\n",
    "    transmat = np.loadtxt(transform)# if isinstance(transform, str) else transform\n",
    "    return np.matmul(vec, transmat)\n",
    "\n",
    "def make_training_matrices(source_dictionary, target_dictionary, bilingual_dictionary):\n",
    "    \"\"\"\n",
    "    Source and target dictionaries are the FastVector objects of\n",
    "    source/target languages. bilingual_dictionary is a list of \n",
    "    translation pair tuples [(source_word, target_word), ...].\n",
    "    \"\"\"\n",
    "    source_matrix = []\n",
    "    target_matrix = []\n",
    "    \n",
    "    len_bd = len(bilingual_dictionary)\n",
    "\n",
    "    for i, (source, target) in enumerate(bilingual_dictionary):\n",
    "        print(f'\\r{i + 1}/{len_bd} | {100 * (i + 1) / len_bd:.3f} %', end = '', flush = True)\n",
    "        sourceVector = source_dictionary.get_sentence_vector(source.lower().strip().replace('_',' '))\n",
    "        targetVector = target_dictionary.get_sentence_vector(target.lower().strip().replace('_',' '))\n",
    "        source_matrix.append(sourceVector)\n",
    "        target_matrix.append(targetVector)\n",
    "        \n",
    "    # return training matrices\n",
    "    return np.array(source_matrix), np.array(target_matrix)\n",
    "\n",
    "def normalized(a, axis=-1, order=2):\n",
    "    \"\"\"Utility function to normalize the rows of a numpy array.\"\"\"\n",
    "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
    "    l2[l2 == 0] = 1\n",
    "    return a / np.expand_dims(l2, axis)\n",
    "\n",
    "\n",
    "def learn_transformation(source_matrix, target_matrix, normalize_vectors=True):\n",
    "    \"\"\"\n",
    "    Source and target matrices are numpy arrays, shape\n",
    "    (dictionary_length, embedding_dimension). These contain paired\n",
    "    word vectors from the bilingual dictionary.\n",
    "    \"\"\"\n",
    "    # optionally normalize the training vectors\n",
    "    if normalize_vectors:\n",
    "        source_matrix = normalized(source_matrix)\n",
    "        target_matrix = normalized(target_matrix)\n",
    "\n",
    "    # perform the SVD\n",
    "    product = np.matmul(target_matrix.transpose(), source_matrix)\n",
    "    U, s, V = np.linalg.svd(product)\n",
    "\n",
    "    # return orthogonal transformation which aligns source language to the target\n",
    "    return np.matmul(U, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pdgQ31teCXWm"
   },
   "outputs": [],
   "source": [
    "# file = ('cx-corpora.en2es.text.json')\n",
    "\n",
    "# with open(file, encoding='utf-8') as f:\n",
    "#     d = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ui-6W_80LmE6",
    "outputId": "7b1782dd-da10-4bcd-ac91-cb1fc207862a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model = {}\n",
    "# model['en'] = fasttext.load_model('wiki.en.bin')\n",
    "# model['es'] = fasttext.load_model('wiki.es.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=[]\n",
    "# for line in d:\n",
    "#     try:\n",
    "#         source = unicodedata.normalize('NFKC',line['source']['content'])\n",
    "#         target = unicodedata.normalize('NFKC',line['target']['content'])\n",
    "#         if not source == '' and not target == '':\n",
    "#             data.append([source,target])\n",
    "#     except TypeError:\n",
    "#         pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QXesC2HkEN5q"
   },
   "outputs": [],
   "source": [
    "## Code to split train and test\n",
    "# df = pd.DataFrame(data,columns = ['source','target'])\n",
    "# train, test = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.to_csv('content_trans_test.csv',index=False)\n",
    "# train.to_csv('content_trans_train.csv',index=False)\n",
    "# test = pd.read_csv('content_test.csv') #same for everyone\n",
    "# train = pd.read_csv('content_train.csv') #replace with your respective dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JciKTIkqD30a"
   },
   "outputs": [],
   "source": [
    "#Train\n",
    "#bilingual_dictionary = list(zip(train['source'],train['target']))\n",
    "#source_matrix, target_matrix = make_training_matrices(model['en'], model['es'], bilingual_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('source_matrix',source_matrix)\n",
    "#np.save('target_matrix',target_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_matrix = np.load('source_matrix.npy')\n",
    "target_matrix = np.load('target_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing transform matrix\n",
    "#transform = learn_transformation(source_matrix, target_matrix)\n",
    "# # ## Save transform as .txt file\n",
    "#np.savetxt('content_trans_transform_en_es.txt', transform)\n",
    "transform = np.loadtxt('content_trans_transform_en_es.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before trans: 0.0008891855\n",
      "After trans: 0.19440520936342\n"
     ]
    }
   ],
   "source": [
    "#before\n",
    "print(\"Before trans:\",np.matmul(target_matrix[:50000, :50000], source_matrix[:50000, :50000].T).mean())\n",
    "#after\n",
    "print(\"After trans:\",np.matmul(target_matrix[:50000, :50000], np.matmul(transform, source_matrix[:50000, :50000].T)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72972/72972 | 100.000 %"
     ]
    }
   ],
   "source": [
    "# #Test\n",
    "# bilingual_dictionary = list(zip(test['source'],test['target']))\n",
    "# source_matrix_test, target_matrix_test = make_training_matrices(model['en'], model['es'], bilingual_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('source_matrix_test',source_matrix_test)\n",
    "# np.save('target_matrix_test',target_matrix_test)\n",
    "source_matrix_test = np.load('source_matrix_test.npy')\n",
    "target_matrix_test = np.load('target_matrix_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before trans: 0.0009806056\n",
      "After trans: 0.19406951518768453\n"
     ]
    }
   ],
   "source": [
    "#before\n",
    "print(\"Before trans:\",np.matmul(target_matrix_test, source_matrix_test.T).mean())\n",
    "#after\n",
    "print(\"After trans:\",np.matmul(target_matrix_test, np.matmul(transform, source_matrix_test.T)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Content Translation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
