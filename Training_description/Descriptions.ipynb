{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dOq1yuJ--zZD"
   },
   "outputs": [],
   "source": [
    "# # !gunzip cx-corpora.en2es.text.json.gz\n",
    "# !pip install --user pybind11\n",
    "# !pip install fasttext\n",
    "#!conda install -c conda-forge fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_w7cb_2j_U-S"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iav225/envs/capstone/lib/python3.6/site-packages/ipykernel_launcher.py:10: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import fasttext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from numpy.core.umath_tests import inner1d\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UsecK2xOBInn"
   },
   "outputs": [],
   "source": [
    "def apply_transform(vec, transform):\n",
    "    \"\"\"\n",
    "    Apply the given transformation to the vector space\n",
    "\n",
    "    Right-multiplies given transform with embeddings E:\n",
    "        E = E * transform\n",
    "\n",
    "    Transform can either be a string with a filename to a\n",
    "    text file containing a ndarray (compat. with np.loadtxt)\n",
    "    or a numpy ndarray.\n",
    "    \"\"\"\n",
    "    transmat = np.loadtxt(transform)# if isinstance(transform, str) else transform\n",
    "    return np.matmul(vec, transmat)\n",
    "\n",
    "def make_training_matrices(source_dictionary, target_dictionary, bilingual_dictionary):\n",
    "    \"\"\"\n",
    "    Source and target dictionaries are the FastVector objects of\n",
    "    source/target languages. bilingual_dictionary is a list of \n",
    "    translation pair tuples [(source_word, target_word), ...].\n",
    "    \"\"\"\n",
    "    source_matrix = []\n",
    "    target_matrix = []\n",
    "    \n",
    "    len_bd = len(bilingual_dictionary)\n",
    "\n",
    "    for i, (source, target) in tqdm(enumerate(bilingual_dictionary)):\n",
    "#         print(f'\\r{i + 1}/{len_bd} | {100 * (i + 1) / len_bd:.3f} %', end = '', flush = True)\n",
    "        sourceVector = source_dictionary.get_sentence_vector(source.lower().strip().replace('_',' '))\n",
    "        targetVector = target_dictionary.get_sentence_vector(target.lower().strip().replace('_',' '))\n",
    "        source_matrix.append(sourceVector)\n",
    "        target_matrix.append(targetVector)\n",
    "        \n",
    "    # return training matrices\n",
    "    return np.array(source_matrix), np.array(target_matrix)\n",
    "\n",
    "def normalized(a, axis=-1, order=2):\n",
    "    \"\"\"Utility function to normalize the rows of a numpy array.\"\"\"\n",
    "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
    "    l2[l2 == 0] = 1\n",
    "    return a / np.expand_dims(l2, axis)\n",
    "\n",
    "\n",
    "def learn_transformation(source_matrix, target_matrix, normalize_vectors=True):\n",
    "    \"\"\"\n",
    "    Source and target matrices are numpy arrays, shape\n",
    "    (dictionary_length, embedding_dimension). These contain paired\n",
    "    word vectors from the bilingual dictionary.\n",
    "    \"\"\"\n",
    "    # optionally normalize the training vectors\n",
    "    if normalize_vectors:\n",
    "        source_matrix = normalized(source_matrix)\n",
    "        target_matrix = normalized(target_matrix)\n",
    "\n",
    "    # perform the SVD\n",
    "    product = np.matmul(target_matrix.transpose(), source_matrix)\n",
    "    U, s, V = np.linalg.svd(product)\n",
    "\n",
    "    # return orthogonal transformation which aligns source language to the target\n",
    "    return np.matmul(U, V)\n",
    "\n",
    "def cleanData(data):\n",
    "    data_cleaned = []\n",
    "    for source, target in zip(data['source_value'], data['target_value']):\n",
    "        try:\n",
    "            source_cleaned = unicodedata.normalize('NFD', source)\n",
    "            target_cleaned = unicodedata.normalize('NFD', target)\n",
    "            if not source == ''  and not target == '':\n",
    "                data_cleaned.append([source_cleaned, target_cleaned])\n",
    "        except:\n",
    "            pass\n",
    "    return data_cleaned\n",
    "\n",
    "\n",
    "def check(source, target):\n",
    "    val = [source, target]\n",
    "    return val != sorted(val)\n",
    "\n",
    "def loadData(source, target):\n",
    "    if check(source, target):\n",
    "        return loadData(target, source)\n",
    "    \n",
    "    file = '{}2{}.csv'.format(source, target)\n",
    "    path = '/scratch/mje341/capstoneWikimedia/Training_description/files/'\n",
    "    files = listdir(path)\n",
    "    retVal = 'description_train_{}'.format(file)\n",
    "    if retVal in files:\n",
    "        return pd.read_csv(os.path.join(path, retVal))\n",
    "    elif file in files:\n",
    "        data = pd.read_csv(os.path.join(path, file), error_bad_lines = False)\n",
    "        data = data.drop(['ID'], axis = 1)\n",
    "        data = data.drop_duplicates()\n",
    "        df = pd.DataFrame(cleanData(data), columns = ['source', 'target'])\n",
    "        df.to_csv(os.path.join(path, retVal))\n",
    "        return pd.read_csv(os.path.join(path, retVal))\n",
    "    else:\n",
    "        raise Exception((\"{0} not in directory {1}\".format(file, path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ui-6W_80LmE6",
    "outputId": "7b1782dd-da10-4bcd-ac91-cb1fc207862a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = {}\n",
    "source_language = 'en'\n",
    "target_language = 'es'\n",
    "model['en'] = fasttext.load_model('/scratch/dev241/capstone/fast/wiki.en.bin')\n",
    "model['es'] = fasttext.load_model('/scratch/dev241/capstone/fast/wiki.es.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('/scratch/ah3243/content_test.csv') #trans_test_en_es.csv') #same for everyone\n",
    "train = loadData('en', 'es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>how the player was acquired; qualifier for P54...</td>\n",
       "      <td>forma en que fue adquirido el jugador; calific...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>rivers and other outflows waterway names. If e...</td>\n",
       "      <td>ri패o que drena el lago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>amount of goods and services bought from other...</td>\n",
       "      <td>cantidad de bienes y servicios comprados a otr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Wikimedia list related to this subject</td>\n",
       "      <td>lista de Wikimedia para el elemento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>identifier for a unique bibliographic record i...</td>\n",
       "      <td>nu패mero de control del Online Computer Library...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             source  \\\n",
       "0           0  how the player was acquired; qualifier for P54...   \n",
       "1           1  rivers and other outflows waterway names. If e...   \n",
       "2           2  amount of goods and services bought from other...   \n",
       "3           3             Wikimedia list related to this subject   \n",
       "4           4  identifier for a unique bibliographic record i...   \n",
       "\n",
       "                                              target  \n",
       "0  forma en que fue adquirido el jugador; calific...  \n",
       "1                             ri패o que drena el lago  \n",
       "2  cantidad de bienes y servicios comprados a otr...  \n",
       "3                lista de Wikimedia para el elemento  \n",
       "4  nu패mero de control del Online Computer Library...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.columns = ['ID','source_lang', 'source', 'target_lang', 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JciKTIkqD30a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "666867it [01:12, 9194.10it/s]\n"
     ]
    }
   ],
   "source": [
    "#Train\n",
    "bilingual_dictionary = list(zip(train['source'],train['target']))\n",
    "source_matrix, target_matrix = make_training_matrices(model['en'], model['es'], bilingual_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = learn_transformation(source_matrix, target_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt('description_trans_transform_en_es.txt', transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before trans: 0.01143723951168304\n"
     ]
    }
   ],
   "source": [
    "print(\"Before trans:\", np.mean(inner1d(target_matrix, source_matrix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After trans: 0.7768910814759667\n"
     ]
    }
   ],
   "source": [
    "print(\"After trans:\", np.mean(inner1d(normalized(target_matrix), np.matmul(transform, normalized(source_matrix).T).T)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "83it [00:00, 822.90it/s]\u001b[A\n",
      "184it [00:00, 870.45it/s]\u001b[A\n",
      "317it [00:00, 970.48it/s]\u001b[A\n",
      "411it [00:00, 955.42it/s]\u001b[A\n",
      "513it [00:00, 971.82it/s]\u001b[A\n",
      "622it [00:00, 1002.71it/s]\u001b[A\n",
      "731it [00:00, 1026.74it/s]\u001b[A\n",
      "829it [00:00, 993.56it/s] \u001b[A\n",
      "927it [00:00, 962.71it/s]\u001b[A\n",
      "1021it [00:01, 913.95it/s]\u001b[A\n",
      "1141it [00:01, 984.10it/s]\u001b[A\n",
      "1247it [00:01, 1005.70it/s]\u001b[A\n",
      "1354it [00:01, 990.34it/s] \u001b[A\n",
      "1454it [00:01, 934.78it/s]\u001b[A\n",
      "1549it [00:01, 932.64it/s]\u001b[A\n",
      "1643it [00:01, 923.85it/s]\u001b[A\n",
      "1736it [00:01, 903.71it/s]\u001b[A\n",
      "1831it [00:01, 914.34it/s]\u001b[A\n",
      "1923it [00:02, 394.47it/s]\u001b[A\n",
      "2049it [00:02, 496.55it/s]\u001b[A\n",
      "2136it [00:02, 516.20it/s]\u001b[A\n",
      "2232it [00:02, 598.34it/s]\u001b[A\n",
      "2339it [00:02, 667.06it/s]\u001b[A\n",
      "2443it [00:03, 747.13it/s]\u001b[A\n",
      "2534it [00:03, 701.86it/s]\u001b[A\n",
      "2646it [00:03, 787.92it/s]\u001b[A\n",
      "2775it [00:03, 890.91it/s]\u001b[A\n",
      "2899it [00:03, 972.29it/s]\u001b[A\n",
      "3009it [00:03, 1006.96it/s]\u001b[A\n",
      "3118it [00:03, 1015.45it/s]\u001b[A\n",
      "3227it [00:03, 1036.71it/s]\u001b[A\n",
      "3342it [00:03, 1065.95it/s]\u001b[A\n",
      "3453it [00:03, 1059.57it/s]\u001b[A\n",
      "3562it [00:04, 871.89it/s] \u001b[A\n",
      "3657it [00:04, 656.12it/s]\u001b[A\n",
      "3781it [00:04, 763.39it/s]\u001b[A\n",
      "3899it [00:04, 852.96it/s]\u001b[A\n",
      "4004it [00:04, 890.46it/s]\u001b[A\n",
      "4104it [00:04, 897.08it/s]\u001b[A\n",
      "4209it [00:04, 935.04it/s]\u001b[A\n",
      "4309it [00:04, 944.71it/s]\u001b[A\n",
      "4408it [00:05, 836.02it/s]\u001b[A\n",
      "4508it [00:05, 874.38it/s]\u001b[A\n",
      "4601it [00:05, 823.46it/s]\u001b[A\n",
      "4692it [00:05, 845.96it/s]\u001b[A\n",
      "4805it [00:05, 913.48it/s]\u001b[A\n",
      "4916it [00:05, 962.93it/s]\u001b[A\n",
      "5018it [00:05, 978.67it/s]\u001b[A\n",
      "5119it [00:05, 950.93it/s]\u001b[A\n",
      "5216it [00:06, 766.76it/s]\u001b[A\n",
      "5317it [00:06, 810.89it/s]\u001b[A\n",
      "5433it [00:06, 889.16it/s]\u001b[A\n",
      "5535it [00:06, 918.19it/s]\u001b[A\n",
      "5632it [00:06, 815.16it/s]\u001b[A\n",
      "5740it [00:06, 879.69it/s]\u001b[A\n",
      "5834it [00:06, 845.69it/s]\u001b[A\n",
      "5954it [00:06, 927.33it/s]\u001b[A\n",
      "6064it [00:06, 973.15it/s]\u001b[A\n",
      "6166it [00:07, 933.56it/s]\u001b[A\n",
      "6263it [00:07, 874.71it/s]\u001b[A\n",
      "6354it [00:07, 873.54it/s]\u001b[A\n",
      "6453it [00:07, 904.16it/s]\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-8949c41b9b46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbilingual_dictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'source'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msource_matrix_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_matrix_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_training_matrices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'es'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbilingual_dictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-84ef8c89c679>\u001b[0m in \u001b[0;36mmake_training_matrices\u001b[0;34m(source_dictionary, target_dictionary, bilingual_dictionary)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbilingual_dictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#         print(f'\\r{i + 1}/{len_bd} | {100 * (i + 1) / len_bd:.3f} %', end = '', flush = True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0msourceVector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource_dictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sentence_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mtargetVector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_dictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sentence_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0msource_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msourceVector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/iav225/envs/capstone/lib/python3.6/site-packages/fasttext/FastText.py\u001b[0m in \u001b[0;36mget_sentence_vector\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetSentenceVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bilingual_dictionary = list(zip(test['source'],test['target']))\n",
    "\n",
    "source_matrix_test, target_matrix_test = make_training_matrices(model['en'], model['es'], bilingual_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#before\n",
    "target_matrix_test = normalized(target_matrix_test)\n",
    "source_matrix_test = normalized(source_matrix_test)\n",
    "\n",
    "print(\"Before trans:\",np.mean(inner1d(target_matrix_test, source_matrix_test)))\n",
    "#after\n",
    "print(\"After trans:\", np.mean(inner1d(target_matrix_test, np.matmul(transform, source_matrix_test.T).T)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Content Translation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
