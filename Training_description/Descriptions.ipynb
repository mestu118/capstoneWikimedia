{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import fasttext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "from tqdm import tqdm\n",
    "from numpy.core.umath_tests import inner1d\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def apply_transform(vec, transform):\n",
    "    \"\"\"\n",
    "    Apply the given transformation to the vector space\n",
    "\n",
    "    Right-multiplies given transform with embeddings E:\n",
    "        E = E * transform\n",
    "\n",
    "    Transform can either be a string with a filename to a\n",
    "    text file containing a ndarray (compat. with np.loadtxt)\n",
    "    or a numpy ndarray.\n",
    "    \"\"\"\n",
    "    transmat = np.loadtxt(transform)# if isinstance(transform, str) else transform\n",
    "    return np.matmul(vec, transmat)\n",
    "\n",
    "def make_training_matrices(source_dictionary, target_dictionary, bilingual_dictionary):\n",
    "    \"\"\"\n",
    "    Source and target dictionaries are the FastVector objects of\n",
    "    source/target languages. bilingual_dictionary is a list of \n",
    "    translation pair tuples [(source_word, target_word), ...].\n",
    "    \"\"\"\n",
    "    source_matrix = []\n",
    "    target_matrix = []\n",
    "    \n",
    "    len_bd = len(bilingual_dictionary)\n",
    "\n",
    "    for i, (source, target) in tqdm(enumerate(bilingual_dictionary)):\n",
    "#         print(f'\\r{i + 1}/{len_bd} | {100 * (i + 1) / len_bd:.3f} %', end = '', flush = True)\n",
    "        sourceVector = source_dictionary.get_sentence_vector(source.lower().strip().replace('_',' '))\n",
    "        targetVector = target_dictionary.get_sentence_vector(target.lower().strip().replace('_',' '))\n",
    "        source_matrix.append(sourceVector)\n",
    "        target_matrix.append(targetVector)\n",
    "        \n",
    "    # return training matrices\n",
    "    return np.array(source_matrix), np.array(target_matrix)\n",
    "\n",
    "def normalized(a, axis=-1, order=2):\n",
    "    \"\"\"Utility function to normalize the rows of a numpy array.\"\"\"\n",
    "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
    "    l2[l2 == 0] = 1\n",
    "    return a / np.expand_dims(l2, axis)\n",
    "\n",
    "\n",
    "def learn_transformation(source_matrix, target_matrix, normalize_vectors=True):\n",
    "    \"\"\"\n",
    "    Source and target matrices are numpy arrays, shape\n",
    "    (dictionary_length, embedding_dimension). These contain paired\n",
    "    word vectors from the bilingual dictionary.\n",
    "    \"\"\"\n",
    "    # optionally normalize the training vectors\n",
    "    if normalize_vectors:\n",
    "        source_matrix = normalized(source_matrix)\n",
    "        target_matrix = normalized(target_matrix)\n",
    "\n",
    "    # perform the SVD\n",
    "    product = np.matmul(target_matrix.transpose(), source_matrix)\n",
    "    U, s, V = np.linalg.svd(product)\n",
    "\n",
    "    # return orthogonal transformation which aligns source language to the target\n",
    "    return np.matmul(U, V)\n",
    "\n",
    "def cleanData(data):\n",
    "    \"\"\"\n",
    "    cleans the sentences in the data given. Possible HTML tags still in the sentences \n",
    "    \"\"\"\n",
    "    \n",
    "    data_cleaned = []\n",
    "    for source, target in zip(data['source_value'], data['target_value']):\n",
    "        try:\n",
    "            source_cleaned = unicodedata.normalize('NFD', source)\n",
    "            target_cleaned = unicodedata.normalize('NFD', target)\n",
    "            if not source == ''  and not target == '':\n",
    "                data_cleaned.append([source_cleaned, target_cleaned])\n",
    "        except:\n",
    "            pass\n",
    "    return data_cleaned\n",
    "\n",
    "def joinData(path, source, target):\n",
    "    \"\"\"\n",
    "    Function loads source.csv and target.csv and creates source2target.csv \n",
    "    \n",
    "    \"\"\"\n",
    "    if check(source, target):\n",
    "        return joinData(target, source)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        df1 = pd.read_csv(os.path.join(path, 'allLanguages', '{}.csv'.format(target)), error_bad_lines = False)\n",
    "    except:\n",
    "        raise Exception(\"{}.csv is not in the directory allLanguages\".format(target))\n",
    "        \n",
    "    try:\n",
    "        df2 = pd.read_csv(os.path.join(path, 'allLanguages', '{}.csv'.format(source)), error_bad_lines = False)\n",
    "    except:\n",
    "        raise Exception(\"{}.csv is not in the directory allLanguages\".format(source))\n",
    "        \n",
    "        \n",
    "    df1.columns = ['ID', 'source_lang', 'source_value']\n",
    "    df2.columns = ['ID', 'target_lang', 'target_value']\n",
    "    \n",
    "    retVal = df1.join(df2.set_index('ID'), on = 'ID')\n",
    "    \n",
    "    retVal.to_csv(os.path.join(path, 'description_files', 'pair_sentences', '{}2{}.csv'.format(source, target)))\n",
    "    \n",
    "    \n",
    "def check(source, target):\n",
    "    \"\"\"\n",
    "    Function to check if souce and target are in sorted order \n",
    "    \"\"\"\n",
    "    \n",
    "    val = [source, target]\n",
    "    return val != sorted(val)\n",
    "\n",
    "def loadData(source, target, recursed = False):\n",
    "    \"\"\"\n",
    "    Function loads the file description_train_source2target.csv\n",
    "    \n",
    "    If file is not found, it create it by loading source2target.csv and cleans data and creates the file\n",
    "    and saves it.\n",
    "    \n",
    "    If source2target.csv is not found. Loads source.csv and target.csv and creates and creates the file\n",
    "    source2target.csv and recurses (recursed = True to avoid infinite recursion).\n",
    "    \"\"\"\n",
    "    \n",
    "    if check(source, target):\n",
    "        return loadData(target, source)\n",
    "    \n",
    "    file = '{}2{}.csv'.format(source, target)\n",
    "    \n",
    "    path = os.getcwd()\n",
    "    \n",
    "    description_path = os.path.join(os.getcwd(), 'description_files')\n",
    "    \n",
    "    train_files = os.listdir(os.path.join(description_path, 'train_files'))\n",
    "    \n",
    "    pair_sentences = os.listdir(os.path.join(description_path, 'pair_sentences'))\n",
    "    \n",
    "    retVal = 'description_train_{}'.format(file)\n",
    "    \n",
    "    if retVal in train_files:\n",
    "        return pd.read_csv(os.path.join(description_path, 'train_files', retVal))\n",
    "    \n",
    "    elif file in pair_sentences:\n",
    "        print(\"Loading from {}\".format(os.path.join(description_path, 'pair_sentences', file)))\n",
    "        \n",
    "        data = pd.read_csv(os.path.join(description_path, 'pair_sentences', file), error_bad_lines = False)\n",
    "        \n",
    "        data = data.drop(['ID'], axis = 1)\n",
    "        data = data.drop_duplicates()\n",
    "        df = pd.DataFrame(cleanData(data), columns = ['source', 'target'])\n",
    "        print(\"Saving at {}\".format(os.path.join(description_path, 'train_files', retVal)))\n",
    "        \n",
    "        df.to_csv(os.path.join(description_path, 'train_files', retVal))\n",
    "        return pd.read_csv(os.path.join(description_path, 'train_files', retVal))\n",
    "    \n",
    "    else:\n",
    "        if recursed:\n",
    "            raise Exception(\"Entered infinite recursion. Check directories in function\")\n",
    "        else:\n",
    "            joinData(path, source, target)\n",
    "            return loadData(source, target, True)\n",
    "        \n",
    "def runTests(source, target):\n",
    "    \n",
    "    if check(source, target):\n",
    "        return runTests(target, source)\n",
    "    \n",
    "    test = pd.read_csv(os.path.join('/scratch', 'ah3243', 'content_test.csv'))\n",
    "    train = loadData(source, target)\n",
    "    \n",
    "    model = {}\n",
    "    path = os.path.join('/scratch', 'dev241', 'capstone', 'fast')\n",
    "    \n",
    "    model[source] = fasttext.load_model(os.path.join(path, 'wiki.{}.bin'.format(source)))\n",
    "    model[target] = fasttext.load_model(os.path.join(path, 'wiki.{}.bin'.format(target)))\n",
    "    \n",
    "    bilingual_dictionary = list(zip(train['source'],train['target']))\n",
    "    \n",
    "    source_matrix, target_matrix = make_training_matrices(model[source], model[target], bilingual_dictionary)\n",
    "    \n",
    "    transform = learn_transformation(source_matrix, target_matrix)\n",
    "    \n",
    "    print(\"Before trans:\", np.mean(inner1d(target_matrix, source_matrix)))\n",
    "    \n",
    "    print(\"After trans:\", np.mean(inner1d(normalized(target_matrix), np.matmul(transform, normalized(source_matrix).T).T)))\n",
    "    \n",
    "    bilingual_dictionary = list(zip(test['source'],test['target']))\n",
    "\n",
    "    source_matrix_test, target_matrix_test = make_training_matrices(model[source], model[target], bilingual_dictionary)\n",
    "    \n",
    "    \n",
    "    target_matrix_test = normalized(target_matrix_test)\n",
    "    source_matrix_test = normalized(source_matrix_test)\n",
    "    \n",
    "    print(\"Before trans:\",np.mean(inner1d(target_matrix_test, source_matrix_test)))\n",
    "    #after\n",
    "    print(\"After trans:\", np.mean(inner1d(target_matrix_test, np.matmul(transform, source_matrix_test.T).T)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "666867it [01:16, 8721.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before trans: 0.01143723951168304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [00:00, 780.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After trans: 0.7768910814759667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4300it [00:05, 938.26it/s] "
     ]
    }
   ],
   "source": [
    "runTests('es', 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Content Translation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
