{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import fasttext\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "- Orthogonal transformation acts on source matrix (or the transpose acts on the target matrix).\n",
    "- Currently `en` is treated as source and `es` is treated as target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transform(vec, transform):\n",
    "    \"\"\"\n",
    "    Apply the given transformation to the vector space\n",
    "\n",
    "    Right-multiplies given transform with embeddings E:\n",
    "        E = E * transform\n",
    "\n",
    "    Transform can either be a string with a filename to a\n",
    "    text file containing a ndarray (compat. with np.loadtxt)\n",
    "    or a numpy ndarray.\n",
    "    \"\"\"\n",
    "    transmat = np.loadtxt(transform)# if isinstance(transform, str) else transform\n",
    "    return np.matmul(vec, transmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_training_matrices(source_dictionary, target_dictionary, bilingual_dictionary):\n",
    "    \"\"\"\n",
    "    Source and target dictionaries are the FastVector objects of\n",
    "    source/target languages. bilingual_dictionary is a list of \n",
    "    translation pair tuples [(source_word, target_word), ...].\n",
    "    \"\"\"\n",
    "    source_matrix = []\n",
    "    target_matrix = []\n",
    "    \n",
    "    len_bd = len(bilingual_dictionary)\n",
    "\n",
    "    for i, (source, target) in enumerate(bilingual_dictionary):\n",
    "        print(f'\\r{i + 1}/{len_bd} | {100 * (i + 1) / len_bd:.3f} %', end = '', flush = True)\n",
    "        sourceVector = source_dictionary.get_sentence_vector(source.lower().strip().replace('_',' '))\n",
    "        targetVector = target_dictionary.get_sentence_vector(target.lower().strip().replace('_',' '))\n",
    "        source_matrix.append(sourceVector)\n",
    "        target_matrix.append(targetVector)\n",
    "        \n",
    "    # return training matrices\n",
    "    return np.array(source_matrix), np.array(target_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized(a, axis=-1, order=2):\n",
    "    \"\"\"Utility function to normalize the rows of a numpy array.\"\"\"\n",
    "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
    "    l2[l2 == 0] = 1\n",
    "    return a / np.expand_dims(l2, axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_transformation(source_matrix, target_matrix, normalize_vectors=True):\n",
    "    \"\"\"\n",
    "    Source and target matrices are numpy arrays, shape\n",
    "    (dictionary_length, embedding_dimension). These contain paired\n",
    "    word vectors from the bilingual dictionary.\n",
    "    \"\"\"\n",
    "    # optionally normalize the training vectors\n",
    "    if normalize_vectors:\n",
    "        source_matrix = normalized(source_matrix)\n",
    "        target_matrix = normalized(target_matrix)\n",
    "\n",
    "    # perform the SVD\n",
    "    product = np.matmul(target_matrix.transpose(), source_matrix)\n",
    "    U, s, V = np.linalg.svd(product)\n",
    "\n",
    "    # return orthogonal transformation which aligns source language to the target\n",
    "    return np.matmul(U, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transform(vec, transform):\n",
    "    \"\"\"\n",
    "    Apply the given transformation to the vector space\n",
    "\n",
    "    Right-multiplies given transform with embeddings E:\n",
    "        E = E * transform\n",
    "\n",
    "    Transform can either be a string with a filename to a\n",
    "    text file containing a ndarray (compat. with np.loadtxt)\n",
    "    or a numpy ndarray.\n",
    "    \"\"\"\n",
    "    transmat = np.loadtxt(transform)# if isinstance(transform, str) else transform\n",
    "    return np.matmul(vec, transmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify depending on your directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join('embeddings', 'fasttext')\n",
    "sentences_path = os.path.join('sentences', 'wikimatrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {}\n",
    "\n",
    "model['en'] = fasttext.load_model(os.path.join(model_path, 'wiki.en.bin'))\n",
    "model['es'] = fasttext.load_model(os.path.join(model_path, 'wiki.es.bin'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WikiMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    with open(os.path.join(sentences_path, 'WikiMatrix.en-es.txt.en'), 'r') as f:\n",
    "        en = f.read()\n",
    "\n",
    "    with open(os.path.join(sentences_path, 'WikiMatrix.en-es.txt.es'), 'r') as f:\n",
    "        es = f.read()\n",
    "\n",
    "    df = pd.DataFrame(zip(en.split('\\n')[:-1], es.split('\\n')[:-1]), columns = ['en', 'es'])\n",
    "    df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('data', 'contenttranslation', 'cx-corpora.en2es.text.json'), 'r') as f:\n",
    "    json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Laura García Benitez (nacida en 19 abril 1981 en Pamplona) es una judoka\\xa0español que ha representado a España en los Juegos Paralímpicos de Verano de 2008 y 2012.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data[3]['target']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Laura García Benitez (nacida en 19 abril 1981 en Pamplona) es una judoka español que ha representado a España en los Juegos Paralímpicos de Verano de 2008 y 2012.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicodedata.normalize('NFKC', json_data[3]['target']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for line in json_data:\n",
    "    try:\n",
    "        source = unicodedata.normalize('NFKC', line['source']['content'])\n",
    "        target = unicodedata.normalize('NFKC', line['target']['content'])\n",
    "        data.append([source, target])\n",
    "    except TypeError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilingual_dictionary = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_matrix, target_matrix = make_training_matrices(model['en'], model['es'], bilingual_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    np.savetxt(os.path.join(sentences_path, 'source.en.txt'), source_matrix)\n",
    "    np.savetxt(os.path.join(sentences_path, 'target.es.txt'), target_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    source_matrix = np.loadtxt(os.path.join(sentences_path, 'source.en.txt'))\n",
    "    target_matrix = np.loadtxt(os.path.join(sentences_path, 'target.es.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = learn_transformation(source_matrix, target_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean cosine similarity before transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matmul(target_matrix[:10000, :10000], source_matrix[:10000, :10000].T).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean cosine similarity after transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matmul(target_matrix[:10000, :10000], np.matmul(transform, source_matrix[:10000, :10000].T)).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
