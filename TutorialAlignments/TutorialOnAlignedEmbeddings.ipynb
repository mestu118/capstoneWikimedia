{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to cross-lingual word-embeddings at Wikimania 2019\n",
    "\n",
    "* In this tutorial we will learn how to work with cross-lingual word embeddings. \n",
    "* See introduction [slides here](https://upload.wikimedia.org/wikipedia/commons/6/63/Tutorial_on_Multilingual_Word_Embeddings%2C_Wikimania_2019.pdf)\n",
    "* This code is based on the repository shared by [Smith et al](https://github.com/Babylonpartners/fastText_multilingual)\n",
    "* You can see applications of these code on the Wikipedia [Sections](https://github.com/digitalTranshumant/wmf-interlanguage) and [Template parameters](https://github.com/digitalTranshumant/templatesAlignment) alignments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Config \n",
    "## Add here your folders and languages\n",
    "import fastText\n",
    "from scipy.spatial import distance\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "lang1 = 'en'\n",
    "lang2 = 'es'\n",
    "langs =[lang1,lang2]\n",
    "pathVectors = 'vectors/' \n",
    "pathAlignment = 'wikiAlignments/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download fasttext models\n",
    "\n",
    "* This script download the fasttext pre-trained models in the languages listed langs variable.\n",
    "* This process **can take long time**.\n",
    "* Note that **each model file is around 8G** , and later you will need to unzip those models, using around 15G per model in total.\n",
    "* Comment (add # prefix)the first line in the next cell to download the models. If you already have the models you in your folder, you can skip this step. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENT  HERE TO RUN THIS CELL\n",
    "\n",
    "!mkdir {pathVectors}\n",
    "for l in lang:\n",
    "    print(l)\n",
    "    !wget -P vectors/ {'https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.%s.zip' % l}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load the models, this can take few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {}\n",
    "for lang in langs:\n",
    "    model[lang] = fastText.load_model('%s/wiki.%s.bin' % (pathVectors,lang))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings example\n",
    "\n",
    "* This is how a cat looks like for this embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03272143,  0.03321843, -0.0772398 ,  0.02752271, -0.04689749,\n",
       "        0.10779523,  0.05039032, -0.1213629 ,  0.00796918,  0.0365316 ,\n",
       "        0.03590076, -0.00070022,  0.04651386, -0.04166288,  0.06664581,\n",
       "       -0.02164455,  0.0180805 , -0.10384931,  0.04688571,  0.06662694,\n",
       "        0.00233572,  0.12208793, -0.09872068,  0.0255165 ,  0.08340995,\n",
       "        0.00577335, -0.0176113 ,  0.06296295,  0.07984982,  0.11208957,\n",
       "        0.06389221,  0.05539172,  0.02762271, -0.05251925,  0.04438546,\n",
       "       -0.02399754, -0.01537215, -0.01010495,  0.01509995, -0.00657111,\n",
       "       -0.02613736, -0.061927  , -0.05292661, -0.00875183,  0.03022415,\n",
       "        0.12282077, -0.01940934, -0.09258875,  0.03871215, -0.06963187,\n",
       "        0.02200041, -0.01411154, -0.02184908, -0.08269455,  0.07468157,\n",
       "        0.08944456,  0.00224687, -0.1002942 ,  0.01784089,  0.04561058,\n",
       "        0.04928856, -0.11202534,  0.02219844,  0.05074738, -0.01451611,\n",
       "       -0.08938298,  0.02949806, -0.00669799, -0.03016053,  0.01116755,\n",
       "       -0.01216495, -0.08206985,  0.05887382, -0.09758892, -0.09325767,\n",
       "        0.00637048, -0.03989533,  0.08270881, -0.08843972,  0.02818453,\n",
       "       -0.01623796,  0.00295207,  0.04470299,  0.08739536,  0.08489413,\n",
       "        0.02700518, -0.03938108,  0.01117819,  0.05708639,  0.01493074,\n",
       "        0.02871869, -0.0460449 ,  0.08188171,  0.03403811,  0.0253881 ,\n",
       "        0.03618389, -0.06597117, -0.05926748, -0.06118818,  0.01171021,\n",
       "        0.04180083,  0.01997853, -0.05869597,  0.02966375,  0.00966474,\n",
       "       -0.05524473,  0.05189364,  0.00175703,  0.06135704, -0.04616855,\n",
       "        0.04846644, -0.00707582, -0.07514237, -0.0123611 ,  0.01133362,\n",
       "       -0.03663448,  0.02257092, -0.04623863,  0.03583612, -0.03202283,\n",
       "       -0.00960319, -0.03316909,  0.01221914,  0.05684921,  0.03675234,\n",
       "        0.03050192,  0.12296707,  0.00666069,  0.07358187,  0.11613067,\n",
       "       -0.07004147,  0.03517091, -0.06014191,  0.07283556,  0.00640122,\n",
       "       -0.02515739,  0.03471669, -0.03982008, -0.10447585,  0.07287212,\n",
       "        0.02370786,  0.02260983,  0.04698145,  0.02574782, -0.01695139,\n",
       "        0.03451049, -0.00160255, -0.04570806, -0.00442773,  0.06822422,\n",
       "       -0.03121714, -0.06248022, -0.10695268, -0.03090054,  0.03447342,\n",
       "        0.02105642,  0.0607292 , -0.02616017, -0.00337311,  0.07103202,\n",
       "        0.02983532,  0.00761606, -0.06654347,  0.03866716,  0.02840243,\n",
       "       -0.0489554 , -0.03658335, -0.0551444 ,  0.02579325,  0.03210133,\n",
       "       -0.01313129, -0.11777922, -0.02994463, -0.0203511 ,  0.03932046,\n",
       "       -0.00641337, -0.01978315, -0.08679112,  0.011424  , -0.04906217,\n",
       "       -0.0420751 , -0.06741455,  0.11509562, -0.07564392, -0.01490594,\n",
       "       -0.0023952 ,  0.00916727, -0.0159545 ,  0.01070366,  0.10338525,\n",
       "       -0.0020056 , -0.08471239, -0.00999477,  0.08227528, -0.08288687,\n",
       "       -0.12098601, -0.10805968, -0.07567748, -0.18051523, -0.03994304,\n",
       "        0.02763269,  0.02266109,  0.02570431,  0.0088365 ,  0.0036729 ,\n",
       "       -0.15515535, -0.00287875, -0.14432205, -0.05114172, -0.06278293,\n",
       "        0.04720113, -0.05614552,  0.04631633, -0.03025362,  0.05448557,\n",
       "       -0.02011943,  0.08389292, -0.00492832, -0.05356852, -0.03316532,\n",
       "       -0.01693997, -0.05033619, -0.12712255,  0.05866379, -0.03195552,\n",
       "       -0.02060165, -0.03451328, -0.00604885,  0.03508535,  0.05694634,\n",
       "        0.03188653,  0.06038615, -0.01133399, -0.01042211,  0.0121483 ,\n",
       "        0.08726177, -0.05743813,  0.03939461, -0.01587104,  0.06706464,\n",
       "       -0.01761667,  0.10062715,  0.07246886,  0.07961714, -0.00538224,\n",
       "        0.00322489, -0.05657806,  0.06032568, -0.0502349 ,  0.12738235,\n",
       "        0.02816762, -0.06601696, -0.1240452 ,  0.0627571 ,  0.02809147,\n",
       "       -0.05813975,  0.04783763, -0.01850602,  0.07075618, -0.05448457,\n",
       "       -0.0044456 ,  0.01098599,  0.01625209, -0.05261958, -0.09557738,\n",
       "       -0.01058379,  0.01890603,  0.00663763,  0.01644058, -0.0195105 ,\n",
       "       -0.04809193, -0.01746241,  0.09031167, -0.09797578,  0.00398914,\n",
       "       -0.14198521, -0.00375971, -0.06100339,  0.08657616,  0.06276353,\n",
       "        0.08580878,  0.01758965,  0.07664976,  0.03455459,  0.09488834,\n",
       "        0.01922096,  0.00618542,  0.04868407, -0.1054813 , -0.03341939,\n",
       "        0.03595594, -0.07743388,  0.02431411, -0.01262254, -0.01633173,\n",
       "       -0.00156663, -0.01580206,  0.06468589,  0.12313337, -0.00206504],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['en'].get_sentence_vector('cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Remember that **those number doesn't have a meaning by themselves**, and will change if you retrain your model in other corpus. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "print(len(model['en'].get_sentence_vector('cat')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distances within the same language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5402991675617295"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = model['en'].get_sentence_vector('cat')\n",
    "v2 = model['en'].get_sentence_vector('kitty')\n",
    "distance.cosine(v1,v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6383497816265513"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = model['en'].get_sentence_vector('cat')\n",
    "v2 = model['en'].get_sentence_vector('lion')\n",
    "distance.cosine(v1,v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8043883760241566"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = model['en'].get_sentence_vector('cat')\n",
    "v2 = model['en'].get_sentence_vector('car')\n",
    "distance.cosine(v1,v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load transformation Matrices\n",
    "* Note that his repository already contains transformation from 'en' to 'es'\n",
    "* This alignments are generated using [this code](https://analytics.wikimedia.org/datasets/one-off/dsaez/)\n",
    "* If you need to a pair of languages that is not contained here, please contact us, or use the pre-trained [provided here](https://drive.google.com/drive/folders/1_cbl3GKmg9Ots6_QOXcGxRNQr8SCELWO?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = model['es'].get_sentence_vector('perro')\n",
    "v2 = model['en'].get_sentence_vector('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9362809884474315"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance.cosine(v1,v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The following function apply the transformation to a given vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transform(vec, transform):\n",
    "        \"\"\"\n",
    "        Apply the given transformation to the vector space\n",
    "\n",
    "        Right-multiplies given transform with embeddings E:\n",
    "            E = E * transform\n",
    "\n",
    "        Transform can either be a string with a filename to a\n",
    "        text file containing a ndarray (compat. with np.loadtxt)\n",
    "        or a numpy ndarray.\n",
    "        \"\"\"\n",
    "        transmat = np.loadtxt(transform) if isinstance(transform, str) else transform\n",
    "        return np.matmul(vec, transmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align second language\n",
    "v2Aligned = apply_transform(v2,'%s/apply_in_%s_to_%s.txt' % (pathAlignment,'en','es') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2947399014085661"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance.cosine(v1,v2Aligned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subword information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using subword information with modified or misspelled words\n",
    "\n",
    "within the same language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42508700188863213"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = model['en'].get_sentence_vector('excellent')\n",
    "v2 = model['en'].get_sentence_vector('excelent')\n",
    "distance.cosine(v1,v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or with cross-lingual aligned vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4684736133135168"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = model['es'].get_sentence_vector('perro1')\n",
    "v2 = model['en'].get_sentence_vector('dog')\n",
    "v2Aligned = apply_transform(v2,'%s/apply_in_%s_to_%s.txt' % (pathAlignment,'en','es') )\n",
    "distance.cosine(v1,v2Aligned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = model['es'].get_sentence_vector('Hola! Que tenga un bonito día señor, nos vemos más tarde! :)')\n",
    "sentence2 = model['en'].get_sentence_vector('Hi! Have a nice day sir, see you later! :)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9927418768618697"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance.cosine(sentence1,sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2Aligned = apply_transform(v2,'%s/apply_in_%s_to_%s.txt' % (pathAlignment,'en','es') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6815586554209534"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance.cosine(sentence1,sentence2Aligned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aligning  sets of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "es\n"
     ]
    }
   ],
   "source": [
    "transmat = {}\n",
    "for lang in langs:\n",
    "    print(lang)\n",
    "    transmat[lang] = {}\n",
    "    for lang2 in langs:\n",
    "        if lang!=lang2:\n",
    "            transmat[lang][lang2] = np.loadtxt('%s/apply_in_%s_to_%s.txt' % (pathAlignment,lang2,lang))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = {}\n",
    "words[lang1] = ['cat','kitty','motocycle','car','dog','truck','geography','mountains','rivers','basketball','football']\n",
    "words[lang2] = ['gato','automóvil','perro','camión','geografía','montañas','rios','baloncesto','futbol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMoreSimilar(wordLang1,setLang2,sourceLang,targetLang):\n",
    "    \"\"\"\n",
    "    Given a word in language 1 and set of words/sentences language 2\n",
    "    return \n",
    "    wordLang1: str, 'perro'\n",
    "    set2: dict or list, ['hello','dog']\n",
    "    sourceLang: str, 'es'\n",
    "    targetLang: str, 'en'\n",
    "    return list\n",
    "    \"\"\"\n",
    "    global model\n",
    "    global transmat\n",
    "    d = []\n",
    "    vec1 = model[sourceLang].get_sentence_vector(wordLang1)\n",
    "    for s2 in setLang2:\n",
    "        vec2= model[targetLang].get_sentence_vector(s2.strip().replace('_',' '))\n",
    "        vec2T = apply_transform(vec2,transmat[sourceLang][targetLang])\n",
    "        dist = distance.cosine(vec1,vec2T)\n",
    "        d.append((dist,s2))\n",
    "    return sorted(d)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for the most similar word to: cat\n",
      "list ['gato', 'automóvil', 'perro', 'camión', 'geografía', 'montañas', 'rios', 'baloncesto', 'futbol']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.43776026412466407, 'gato')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordEn ='cat'\n",
    "print('Searching for the most similar word to:', wordEn)\n",
    "print('list',words['es'])\n",
    "getMoreSimilar(wordEn,words['es'],'en','es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for the most similar word: kitty\n",
      "list ['gato', 'automóvil', 'perro', 'camión', 'geografía', 'montañas', 'rios', 'baloncesto', 'futbol']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.647667168023605, 'gato')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordEn ='kitty'\n",
    "print('Searching for the most similar word:', wordEn)\n",
    "print('list',words['es'])\n",
    "getMoreSimilar(wordEn,words['es'],'en','es')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aligning set of words\n",
    "\n",
    "Given two sets of words, get a mapping one-to-one mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-to-one mappings\n",
    "def alignSets(set1,set2,sourceLang,targetLang,sensivity=.45):\n",
    "    \"\"\"\n",
    "    Given two sets of words/sentences in two languages\n",
    "    return the possible alignments between sentences\n",
    "    set1: dict or list, ['hola','perro']\n",
    "    set2: dict or list, ['hello','dog']\n",
    "    sourceLang: str, 'es'\n",
    "    targetLang: str, 'en'\n",
    "    return list\n",
    "    \"\"\"\n",
    "    global model\n",
    "    global transmat\n",
    "    output = []\n",
    "    G= nx.Graph()\n",
    "    for s1 in set1:\n",
    "        vec1 = model[sourceLang].get_sentence_vector(s1.strip().replace('_',' '))\n",
    "        for s2 in set2:\n",
    "                    vec2= model[targetLang].get_sentence_vector(s2.strip().replace('_',' '))\n",
    "                    vec2T = apply_transform(vec2,transmat[sourceLang][targetLang])\n",
    "                    dist = distance.cosine(vec1,vec2T)\n",
    "                    if dist < sensivity:\n",
    "                        node1= '%s_%s' % (sourceLang,s1)\n",
    "                        node2= '%s_%s' % (targetLang,s2)\n",
    "                        G.add_edge(node1,node2)\n",
    "                        G[node1][node2]['w'] = dist\n",
    "\n",
    "                \n",
    "    while G.edges():\n",
    "            p = sorted(G.edges(data=True), key=lambda x: x[2]['w'])[0]\n",
    "            psorted = sorted(list(p[:2]))\n",
    "            output.append({psorted[0][:2]:psorted[0][3:],psorted[1][:2]:psorted[1][3:],'d':p[2]['w']})\n",
    "            G.remove_node(p[0])\n",
    "            G.remove_node(p[1])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'kitty', 'motocycle', 'car', 'dog', 'truck', 'geography', 'mountains', 'rivers', 'basketball', 'football']\n",
      "['gato', 'automóvil', 'perro', 'camión', 'geografía', 'montañas', 'rios', 'baloncesto', 'futbol']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'d': 0.20396928012967108, 'en': 'basketball', 'es': 'baloncesto'},\n",
       " {'d': 0.2424814900096134, 'en': 'mountains', 'es': 'montañas'},\n",
       " {'d': 0.27150193662130007, 'en': 'truck', 'es': 'camión'},\n",
       " {'d': 0.2744324328139399, 'en': 'car', 'es': 'automóvil'},\n",
       " {'d': 0.2938059497380646, 'en': 'geography', 'es': 'geografía'},\n",
       " {'d': 0.29473989913984433, 'en': 'dog', 'es': 'perro'},\n",
       " {'d': 0.3924369271143857, 'en': 'football', 'es': 'futbol'},\n",
       " {'d': 0.43776026412466407, 'en': 'cat', 'es': 'gato'}]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(words[lang1])\n",
    "print(words[lang2])\n",
    "\n",
    "alignSets(words[lang1],words[lang2],lang1,lang2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
